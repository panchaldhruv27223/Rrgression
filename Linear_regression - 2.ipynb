{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6132e91a",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d611a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-square:\n",
    "    r-square is a statistical concept used to measure how well model predict the outcome.\n",
    "    \n",
    "r2 values lies between 0 to 1.\n",
    "when model perfactly predict the data. there is a no diff between orignal data and predicted data.\n",
    "we get r2 equals 0 when the model does not predict any variability in the model and it does not learn \n",
    "any relationship between the dependent and independent variables.\n",
    "r2 value can goes negative when model predict the data is worse than average fitted model.\n",
    "\n",
    "r2 = 1 - ssr/sstotal\n",
    "\n",
    "ssr = sum of square residuals (y - y_predict)**2\n",
    "sstotal = sum of square total (y-y_mean)**2\n",
    "\n",
    "r2 is a comparison of the residual sum of squares (SSres) with the total sum of squares(SStot). \n",
    "\n",
    "The residual sum of squares is calculated by the summation of squares of perpendicular distance between data points\n",
    "and the best-fitted line. \n",
    "The total sum of squares is calculated by summation of squares of perpendicular distance between \n",
    "data points and the average line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00053c",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted r2 is nothing but updated version of r2.\n",
    "it take the value of r2,number of observation,number of independent variable.\n",
    "\n",
    "adjusted r2 = 1  - (1-r2)*(n-1)/(n-k-1)\n",
    "n is the total number of observations in the data.\n",
    "k is the number of independent variables (predictors) in the regression model.\n",
    "\n",
    "r2 value is always increace when we add new feature. so we are not able to determine whether this feature is really \n",
    "important or not.\n",
    "but in case of adjusted r2 it decrease the value when we add some irrelavent feature.\n",
    "\n",
    "adjusted r2 value is always small then r2 value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9f92f",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted r2 give us a value which show how well model predict the unseen data.\n",
    "but there is a one more thing adjusted r2 is also give us a value which is increase if we add some effective variable or \n",
    "feature, and it is also decrease when we adde some irrelavent feature.\n",
    "\n",
    "so in this type of senario it is good to use adjusted r2 rather than r2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28879c99",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25889a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse,mse,mae are all the thing which is used to measure error of the model.\n",
    "error is nothing but the diff between actual value and predicted value.\n",
    "\n",
    "mse : mean square error : (1/n*(sum(y(i)-ypredict**2))\n",
    "mae : mean absolute error : (1/n*(sum(abs(y(i)-ypredict))))\n",
    "rmse : root mean square error : (1/n*(sum(y(i)-ypredict**2))**0.5\n",
    "                                 i --> 1 to n\n",
    "                                 \n",
    "These metrics tell us how accurate our predictions are and, what is the amount of deviation from the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e827ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE :\n",
    "The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the \n",
    "dataset. It measures the average of the residuals in the dataset.\n",
    "\n",
    "MSE :\n",
    "Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. \n",
    "It measures the variance of the residuals.\n",
    "\n",
    "RMSE:\n",
    "Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efb05a",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages of MSE :\n",
    "    1. eqvation is differentiabal.\n",
    "    2. it has only one global or local minimun.\n",
    "    \n",
    "disadvantages of MSE :\n",
    "    1. it is not robust to outliers.\n",
    "    2. it is not in the same unit.\n",
    "    \n",
    "advantages of RMSE :\n",
    "    1. eqvation is differentiabal.\n",
    "    2. it has only one global or local minimun.\n",
    "    3. it is on same unit.\n",
    "    \n",
    "disadvantages of RMSE :\n",
    "    1. it is not robust to outliers.\n",
    "    \n",
    "advantages of MAE :\n",
    "    1. it is on same unit.\n",
    "    2. it is robust to the outliers.\n",
    "    \n",
    "disadvantages of MAE :\n",
    "    1. convergance usually take more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e86eb",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631641b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso stand for : Least Absolute Shrinkage and Selection Operator.\n",
    "    \n",
    "lasso and ridge both are used for regularization.\n",
    "in lasso penalty : lambda*(sum(|b|))\n",
    "in ridge penalty : lambda*(sum(b**2))\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression and other \n",
    "regression-based machine learning algorithms to prevent overfitting and improve model generalization. It does this by adding\n",
    "a penalty term to the standard linear regression loss function, which encourages the model to shrink the coefficients of less\n",
    "important features toward zero. This leads to feature selection, as some coefficients may become exactly zero, effectively\n",
    "removing those features from the model.\n",
    "\n",
    "loss = MSE(cost value) + lambda*(sum(b(i))) \n",
    "\n",
    "the main diff between lesso and ridge is there penalty value, and the another diff is in lesso the coefficent value can be = 0\n",
    "means we are ignoring that feature. in ridge this is not happend, ridge simply reduce the coeffecent(slop value) of variable\n",
    "but it is not equal to zero.\n",
    "\n",
    "Lasso regularization is more appropriate when you have a high-dimensional dataset with many features, and you suspect that some\n",
    "of those features are irrelevant or redundant. Lassoss feature selection capability helps in creating a simpler model by \n",
    "eliminating less important features. It can be particularly effective when you want to identify a smaller subset of features \n",
    "that have the most predictive power, leading to a more interpretable and potentially more accurate model.\n",
    "\n",
    "Additionally, if you are looking to build a sparse model where only a few features are expected to contribute significantly to\n",
    "the outcome, Lasso is a good choice. Its also suitable when you have limited samples compared to the number of features, \n",
    "as Lassos feature selection can help mitigate the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9105b",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cb005",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models, such as Ridge and Lasso regression, help prevent overfitting in machine learning by adding a penalty\n",
    "term to the standard linear regression loss function. This penalty term discourages the model from fitting the training data \n",
    "too closely and from assigning overly large coefficients to the features. As a result, regularized linear models promote \n",
    "simpler and more generalizable models that are less likely to overfit the training data.\n",
    "\n",
    "Lets illustrate this with an example using Ridge regression:\n",
    "\n",
    "Suppose you have a dataset of housing prices with the following features: \n",
    "    square footage, number of bedrooms, number of bathrooms, and age of the house.\n",
    "\n",
    "You want to build a regression model to predict the price of a house based on these features.\n",
    "\n",
    "Standard Linear Regression:\n",
    "\n",
    "In standard linear regression, the model aims to minimize the Mean Squared Error (MSE) between the predicted and actual house\n",
    "prices. It might fit the training data well, even capturing the noise and fluctuations in the data. However, this can lead to \n",
    "overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "Ridge Regression:\n",
    "\n",
    "Ridge regression adds a penalty term to the loss function, which is proportional to the sum of the squared coefficients of the\n",
    "features. The objective is to minimize the sum of the MSE and the squared magnitudes of the coefficients, scaled by a parameter \n",
    "lambda:\n",
    "\n",
    "Loss = MSE + lambda*(sum((bi)**2))\n",
    "\n",
    "where bi are the coefficients of the features.\n",
    "\n",
    "The effect of the penalty term is that it discourages the model from assigning very large values to the coefficients. \n",
    "This leads to a smoother and more regularized solution. The parameter lambda controls the strength of the regularization. \n",
    "As lambda increases, the impact of the penalty term becomes stronger.\n",
    "\n",
    "Example:\n",
    "\n",
    "In our housing price prediction example, let's say that the standard linear regression model assigns large coefficients to \n",
    "all the features, including some that might not have strong predictive power (e.g., noise). This can lead to overfitting, \n",
    "where the model captures the noise in the training data and doesn't generalize well to new data.\n",
    "\n",
    "On the other hand, if we apply Ridge regression with an appropriate value of lambda, the model will be incentivized to reduce \n",
    "the magnitudes of the coefficients. This can lead to a model that assigns smaller weights to less important features, \n",
    "effectively filtering out noise and preventing overfitting. The Ridge regularization helps strike a balance between fitting \n",
    "the training data and avoiding overly complex models.\n",
    "\n",
    "In summary, regularized linear models like Ridge regression help prevent overfitting by introducing a penalty term that \n",
    "encourages the model to produce simpler and more generalized solutions. The regularization parameter controls the trade-off\n",
    "between fitting the training data closely and avoiding overly complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fccf4",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "While regularized linear models like Ridge and Lasso regression offer many benefits in preventing overfitting and improving \n",
    "model generalization, they also have certain limitations that may make them less suitable for specific regression analysis \n",
    "scenarios. Here are some limitations of regularized linear models:\n",
    "    1. Loss of Interpretability\n",
    "    2. Feature Scaling Dependency\n",
    "    3. Model Selection Difficulty(selection of alpha value)\n",
    "    4. Not Ideal for Complex Interactions\n",
    "    5. High-Dimensional Data\n",
    "    6. Violation of Assumptions\n",
    "    7. Bias-Variance Trade-off\n",
    "    8. Sparse Solutions Might Not Always Be Appropriate\n",
    "    9. Limited to Linear Models\n",
    "    \n",
    "while regularized linear models are powerful tools for regression analysis, they may not always be the best choice in every \n",
    "situation. It's important to carefully consider the specific characteristics of the data, the underlying relationships, and \n",
    "the goals of the analysis before deciding whether to use regularized linear models or explore alternative approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e9309",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics.Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb425a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of comparing the performance of regression models, both RMSE (Root Mean Square Error) and \n",
    "MAE (Mean Absolute Error) are common evaluation metrics, but they emphasize different aspects of the model's performance.\n",
    "\n",
    "RMSE (Root Mean Square Error):\n",
    "RMSE gives more weight to larger errors due to squaring, making it sensitive to outliers. \n",
    "A lower RMSE indicates that the model's predictions are, on average, closer to the actual values. In your case, Model \n",
    "A has an RMSE of 10, meaning that, on average, its predictions deviate from the actual values by approximately 10 units.\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "MAE gives equal weight to all errors and is less sensitive to outliers. A lower MAE indicates that the model's \n",
    "predictions have less absolute deviation from the actual values. In your case, Model B has an MAE of 8, meaning that, \n",
    "on average, its predictions deviate from the actual values by approximately 8 units.\n",
    "\n",
    "Choice of Better Model:\n",
    "Given the provided metrics, Model B with an MAE of 8 is performing better than Model A with an RMSE of 10. \n",
    "This suggests that, on average, the predictions of Model B are closer to the actual values compared to the predictions \n",
    "of Model A.\n",
    "\n",
    "Limitations and Considerations:\n",
    "While choosing the better model based on the provided metrics is reasonable, there are some limitations and considerations \n",
    "to keep in mind:\n",
    "\n",
    "1. Sensitivity to Outliers: RMSE is more sensitive to outliers than MAE due to the squaring of errors. \n",
    "    If your dataset contains significant outliers, the RMSE of Model A might be disproportionately affected,\n",
    "    potentially making Model B a more robust choice.\n",
    "\n",
    "2. Interpretability: MAE is more interpretable since it directly represents the average absolute deviation.\n",
    "    RMSE, being squared, is not in the same units as the original data, which might affect its interpretability.\n",
    "\n",
    "3. Business Context: The choice between RMSE and MAE should also be influenced by the specific business or domain context.\n",
    "    Some scenarios may prioritize minimizing larger errors (RMSE), while others may be more concerned with reducing all \n",
    "    errors (MAE).\n",
    "\n",
    "4. Trade-offs: Lower RMSE or MAE values don't necessarily indicate a perfect model. \n",
    "    There is often a trade-off between bias and variance. A model that minimizes RMSE or MAE might overfit or underfit the data,\n",
    "    respectively. It's crucial to assess the model's performance on validation or test data.\n",
    "\n",
    "5. Other Metrics: RMSE and MAE are just two of many possible evaluation metrics. Depending on the problem and goals, you might\n",
    "    want to consider other metrics such as Mean Absolute Percentage Error (MAPE), R-squared, or domain-specific metrics.\n",
    "\n",
    "\n",
    "    \n",
    "so based on the provided metrics, Model B with the lower MAE of 8 seems to be the better performer. However, it's \n",
    "important to consider the limitations of the chosen metric and make a decision in the broader context of the problem you're\n",
    "trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef899d",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model Buses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "When comparing the performance of two regularized linear models using different types of regularization (Ridge and Lasso), \n",
    "there are several factors to consider. The choice of the better performer depends on the specific characteristics of the data, \n",
    "the goals of the analysis, and the trade-offs associated with each type of regularization.\n",
    "\n",
    "Model A: Ridge Regularization with lambda = 0.1\n",
    "\n",
    "Ridge regularization adds a penalty term proportional to the sum of the squared coefficients  to the loss \n",
    "function. The parameter lambda controls the strength of the regularization. Smaller values of lambda result in weaker \n",
    "regularization.\n",
    "\n",
    "Model B: Lasso Regularization with lambda = 0.5\n",
    "\n",
    "Lasso regularization adds a penalty term proportional to the sum of the absolute values of the coefficients to the loss \n",
    "function. The parameter lambda controls the strength of the regularization. Lasso has the feature selection property,\n",
    "meaning it can drive some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "Choosing the Better Model:\n",
    "\n",
    "To determine the better performer between Model A and Model B, you would typically compare their performance using a suitable \n",
    "evaluation metric on a validation or test dataset. Common metrics include Mean Squared Error (MSE), Mean Absolute Error (MAE),\n",
    "Root Mean Squared Error (RMSE), or other domain-specific metrics. The model with the lower value of the chosen metric is\n",
    "generally considered the better performer.\n",
    "\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "1. Feature Selection:\n",
    "   - Ridge: Ridge regularization tends to shrink coefficients towards zero, but they rarely become exactly zero. It doesn't\n",
    "    perform explicit feature selection.\n",
    "   - Lasso: Lasso can drive some coefficients to exactly zero, effectively performing feature selection. This is advantageous\n",
    "    if you believe some features are irrelevant or redundant.\n",
    "\n",
    "2. Interpretability:\n",
    "   - Ridge: Ridge maintains all features and reduces their impact, making it more suitable when you want to preserve \n",
    "    interpretability and include all features.\n",
    "   - Lasso: Lasso can make some coefficients exactly zero, leading to a more interpretable model by removing less important\n",
    "    features.\n",
    "\n",
    "3. Multicollinearity:\n",
    "   - Ridge: Ridge helps mitigate multicollinearity (high correlation between features) by reducing the impact of correlated\n",
    "    features.\n",
    "   - Lasso: Lasso can lead to more extreme feature selection when dealing with correlated features, which may or may not be\n",
    "    desirable.\n",
    "\n",
    "4. Complexity:\n",
    "   - Ridge: Ridge may work well when you want to prevent overfitting by reducing the impact of large coefficients, but you \n",
    "    don't necessarily want to eliminate any features.\n",
    "   - Lasso: Lasso can be particularly effective when you want to simplify the model by explicitly excluding certain features.\n",
    "\n",
    "5. Parameter Tuning:\n",
    "   - The choice of lambda is critical for both regularization methods. Proper parameter tuning (via techniques like \n",
    "    cross-validation) is essential to ensure optimal model performance.\n",
    "\n",
    "In summary, the choice between Ridge and Lasso regularization depends on the specific goals of your analysis. \n",
    "Ridge is suitable when you want to reduce overfitting and control the impact of large coefficients without eliminating features.\n",
    "Lasso is suitable when you want feature selection and a simpler, more interpretable model, potentially at the cost of excluding\n",
    "some features. Ultimately, the better performer should be determined through empirical evaluation using appropriate metrics and \n",
    "considering the context of the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
