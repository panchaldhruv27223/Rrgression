{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390b8160",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression is nothing but Oridnal least square regression + some penalty \n",
    "Ridge try to make some error. it try to reduce over fitting that why we add penalty.\n",
    "this penalty is alpha*(slop**2)\n",
    "we add this to mse\n",
    "so ridge will never zero it try to reduce mse as well as penalty but never Zero so it prevent from overfitting\n",
    "\n",
    "osl regression is a simple regression which try to reduce mse.\n",
    "and find the best fit line for our data, their is a high chance that this line will over fit the data.\n",
    "so this is why ridge and lasso come in a picture.\n",
    "ridge regression is a regularized version of ols regression.\n",
    "it is also called as l2 norm.\n",
    "\n",
    "the main diff between this two is a penalty.\n",
    "ols regression only have mse \n",
    "ridge have mse+penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef202dc5",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linearity: \n",
    "    As with ordinary least squares (OLS) regression, Ridge Regression assumes that the relationship between the predictor\n",
    "    variables and the response variable is linear. This means that changes in the predictor variables are associated with\n",
    "    proportional changes in the response variable.\n",
    "\n",
    "Independence: \n",
    "    The observations are assumed to be independent of each other. This assumption is important to avoid bias in the coefficient\n",
    "    estimates.\n",
    "\n",
    "Homoscedasticity: \n",
    "    The variance of the errors (residuals) should be constant across all levels of the predictor variables. This assumption \n",
    "    ensures that the models predictions have consistent variability across the data.\n",
    "\n",
    "Multicollinearity Awareness: \n",
    "    Ridge Regression specifically addresses the assumption of multicollinearity, which is the presence of high correlation \n",
    "    between predictor variables. It assumes that multicollinearity might exist in the data and aims to mitigate its impact by\n",
    "    introducing a regularization term.\n",
    "\n",
    "Normality of Residuals: \n",
    "    Unlike some other regression techniques, Ridge Regression doesn't assume that the residuals (differences between observed \n",
    "    and predicted values) are normally distributed. This is because the regularization term affects the estimation of \n",
    "    coefficients, not the distribution of residuals.\n",
    "\n",
    "Zero-Centered Predictors: \n",
    "    Ridge Regression assumes that the predictor variables have been centered (i.e., subtracted by their mean) before applying\n",
    "    the regularization. This ensures that the regularization term doesn't in advertently penalize certain variables more than\n",
    "    others due to differences in scale.\n",
    "\n",
    "Regularization Strength Parameter Choice: \n",
    "    Ridge Regression assumes that an appropriate value for the regularization strength parameter (often denoted as lambda or \n",
    "    alpha) is chosen. This parameter controls the trade-off between fitting the data well and preventing overfitting.\n",
    "\n",
    "No Perfect Collinearity:\n",
    "    Ridge Regression assumes that there is no perfect collinearity in the data, meaning that one predictor variable cannot be \n",
    "    expressed as a linear combination of other predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8477d",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "as we know lambda present the strength of the model.\n",
    "we can select the lambda value by hypertuning test : Grid search cv, random search cv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3d45d",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c78957",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge is try to reduce features coefficent but not equal to Zero.\n",
    "so full feature is not ignore.\n",
    "ridge more usually used for redusing over fitting not for feature selection for feature selection we can use lasso \n",
    "regresson.\n",
    "\n",
    "we can indirectly use ridge as feature selection like this:\n",
    "    Ridge Regression with Cross-Validation: \n",
    "        Perform Ridge Regression with cross-validation using a range of lambda values (the regularization parameter). As \n",
    "        lambda increases, the regularization effect becomes stronger, leading to coefficients being shrunken towards zero.\n",
    "\n",
    "    Analyze Coefficient Magnitudes:\n",
    "        During cross-validation, observe how the magnitudes of the coefficients change as lambda varies. \n",
    "        Some coefficients will shrink more rapidly towards zero than others. Features associated with coefficients that \n",
    "        become very small for higher lambda values might be considered less important by the model.\n",
    "\n",
    "    Select an Appropriate Lambda: \n",
    "        Use cross-validation results to determine an appropriate lambda value that balances model fit and complexity. \n",
    "        A higher lambda value will lead to more aggressive coefficient shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6343661",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression model is made to perfrom in multicollinearity.\n",
    "when we have fearues which are more corelated then we can determine there indivisual effect on output feature.\n",
    "then this probelm is called multicollinearity, to address this issue we use ridge regression.\n",
    "ridge regression minimize there coeffecent so it reduce their effect, then we can easly determine indivisual effect.\n",
    "in ridge regresson we use penalty term to minimize there coefficent.\n",
    "in penalty we use lambda value which represent the stregnth of the regulraization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6daca",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression can handle categorical as well as continuous independent variable.\n",
    "in continuous varoable there is a no problem but when we use categorical independent variable then we have to first convert it \n",
    "into continuous variable. for that we can use various transfromation techniques like one hot encoding, label encoding..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85a73d",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In ridge regression we tryto minimize the coefficents and regulraize the feature. \n",
    "so in ridge regression the coefficient of ridge regression depend on the Mse + penalty term(lambda).\n",
    "\n",
    "Magnitude and Sign: \n",
    "    Just like in ordinary linear regression, the sign of a coefficient in Ridge Regression indicates the direction of the \n",
    "    relationship between that predictor and the response variable. A positive coefficient means that an increase in the \n",
    "    predictor's value is associated with an increase in the response variable's value, while a negative coefficient indicates \n",
    "    the opposite. \n",
    "    The magnitude of the coefficient represents the strength of this relationship.\n",
    "\n",
    "Shrinkage towards Zero: \n",
    "    The primary difference between Ridge Regression and ordinary linear regression is the addition of a regularization term.\n",
    "    This term encourages the coefficients to be small, including potentially shrinking them towards zero.\n",
    "    The larger the value of lambda, the more aggressive the shrinkage. Therefore, in Ridge Regression, you may observe that \n",
    "    coefficients are generally smaller compared to ordinary linear regression.\n",
    "\n",
    "Relative Importance:\n",
    "    The relative importance of predictors in Ridge Regression can be inferred from the magnitude of the coefficients after \n",
    "    considering the regularization effect. Features with larger coefficients (even after shrinkage) can still be considered \n",
    "    relatively more important in influencing the response variable.\n",
    "\n",
    "Comparative Interpretation: \n",
    "    When comparing the magnitudes of coefficients across predictors, it's the relative differences that are more important \n",
    "    than the absolute values. Features with larger coefficients compared to others might have a relatively stronger influence\n",
    "    on the response, even if their absolute values are still moderate due to regularization.\n",
    "\n",
    "Interaction with Regularization Strength:\n",
    "    As you vary the regularization strength (lambda), the coefficients will change accordingly. Smaller lambda values allow for\n",
    "    less shrinkage, potentially leading to coefficients that closely resemble those in ordinary linear regression.\n",
    "    Larger lambda values will lead to coefficients being shrunken towards zero more aggressively.\n",
    "\n",
    "Scaling of Features: \n",
    "    Remember that Ridge Regression is sensitive to the scaling of the features. If your features are on different scales,\n",
    "    their coefficients might be influenced more by their magnitudes rather than their actual importance. \n",
    "    It's recommended to scale the features before applying Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e07570",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Ridge Regression can indeed be used for time-series data analysis. Ridge Regression is a variant of linear regression that \n",
    "introduces a regularization term to the standard linear regression objective function. This regularization term helps prevent\n",
    "overfitting by adding a penalty based on the magnitude of the coefficients. In the context of time-series data analysis, Ridge\n",
    "Regression can be beneficial in certain situations.\n",
    "\n",
    "The objective function of Ridge Regression involves minimizing the sum of squared errors while also penalizing the magnitude \n",
    "of the coefficients. This helps in preventing coefficients from becoming too large and overfitting the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
